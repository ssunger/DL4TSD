{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Modelling for the TS based Cov-Matrix Picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "from math import factorial as fac\n",
    "import matplotlib\n",
    "import random as rd\n",
    "from random import sample \n",
    "#import matplotlib.pyplot as plt \n",
    "from itertools import combinations as combs\n",
    "from numpy import random as npr\n",
    "import torch.nn as nn\n",
    "#import torch.distributed as dist\n",
    "#from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nChoosek(n,k):\n",
    "    return fac(n) // fac(k) // fac(n-k)\n",
    "\n",
    "def which_mat(TTS, choice, n = [5, 5*3, 5*5, 5*9, 5*13], p = [0], channel_use = 1):\n",
    "    \"\"\"\n",
    "    Funciton which can be easilly calle din order to implament which_mat as part of a larger function\n",
    "    \"\"\"\n",
    "    return get_mat(TTS, n[choice[0]], p[0], channel_use)\n",
    "\n",
    "def get_mat(TTS, n,p = 0, channel_use = 0):\n",
    "    \"\"\"\n",
    "    Funciton that takes in \n",
    "    TTS Tesnor of Time Serieses, and given \n",
    "    n number of period to use and\n",
    "    p  last index period to use, \n",
    "    return a covariance matrix of the data for that period in terms of the values in chanel channel_use\n",
    "    \"\"\"\n",
    "    \n",
    "    d1, d2 = TTS[0], TTS[1]\n",
    "    #insert function to partition TTS correctly\n",
    "    \n",
    "    #creating cov matrix estimation desired\n",
    "    d = [d1[-n:,channel_use], d1[-n:,channel_use]]\n",
    "    return pd.DataFrame({\"S%d\" %i: d for i,d in  enumerate(d)}).cov()\n",
    "\n",
    "\n",
    "def cov_matrix_loss(A,B, type_use = 1):\n",
    "    \"\"\"\n",
    "    Get distance between matracies based either on component wise distance or using eigenvalues\n",
    "    \"\"\"\n",
    "    if type_use == 0:\n",
    "        ind = np.triu_indices(2)\n",
    "        loss = nn.MSELoss()\n",
    "        return loss(A[ind], B[ind]) \n",
    "    \n",
    "    elif type_use == 1:\n",
    "        return torch.sqrt(torch.sum(torch.pow(torch.log(torch.eig(A-B))),2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_based_simple(nn.Module):\n",
    "    def __init__(self, num_outs, kernel = 5, in_channels = 4,  hidden_size = 130):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_simple, self).__init__()\n",
    "        \n",
    "        pads = int(kernel // 2)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = hidden_size,kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = int(hidden_size/2), \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/2), out_channels = int(hidden_size/(2**2)), \n",
    "                      kernel_size = int(kernel-2), padding = (pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/(2**2))),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconvLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(num_outs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(in_features = num_outs, out_features = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.conv1(input_1)\n",
    "        self.c12 = self.conv2(self.c11)\n",
    "        self.c13 = self.conv3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.conv1(input_2)\n",
    "        self.c22 = self.conv2(self.c11)\n",
    "        self.c23 = self.conv3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        \n",
    "\n",
    "class TS_based_wfactors(nn.Module):\n",
    "    def __init__(self, num_outs, kernel = 5, channels_in =4, hidden_size = 130):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_wfactors, self).__init__()\n",
    "        \n",
    "        pads  = int(kernel//2)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = channels_in, out_channels = hidden_size,\n",
    "                      kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = int(hidden_size/2), \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/2), out_channels = int(hidden_size/(2**2)), \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(int(hidden_size/(2**2))),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.convLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(num_outs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(in_channels = num_outs , out_channels = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.conv1(input_1)\n",
    "        self.c12 = self.conv2(self.c11)\n",
    "        self.c13 = self.conv3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.conv1(input_2)\n",
    "        self.c22 = self.conv2(self.c11)\n",
    "        self.c23 = self.conv3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asset_combinations(x,y):\n",
    "    t = [0,0,0,1]\n",
    "    \"\"\"\n",
    "    Notice:\n",
    "        if x = SP400 then t[0] or t[1] = 1\n",
    "        if y = SP500 then t[2] or t[1] = 1\n",
    "    \n",
    "    \"\"\"\n",
    "    if x == y:\n",
    "        t[3] = 0\n",
    "        if x == \"SP400\":\n",
    "            t[0] = 1\n",
    "        else:\n",
    "            t[2] = 1\n",
    "    elif x == \"SP400\":\n",
    "        t[3] = 0\n",
    "        t[1] = 1\n",
    "    return t\n",
    "    \n",
    "def create_data_sets(list_periods, list_combs, data_use):\n",
    "    type_1 = data_use[0][0].keys()\n",
    "    \n",
    "    #which indecies does each asset bellong to \n",
    "    which_type = lambda x: \"SP400\" if x in type_1 else \"SP500\"\n",
    "    met_dat = [[asset_combinations(which_type(a[0]), which_type(a[1]))] for a in list_combs]\n",
    "    which_type = lambda x: 0 if x in type_1 else 1\n",
    "    name_dat = [[which_type(a[0]), which_type(a[1])] for a in list_combs]\n",
    "    dater_test = []\n",
    "    dater_train = []\n",
    "\n",
    "    checker_n = lambda x,y,j :  x <= min(len(y[name_dat[j][0]][0][list_combs[j][0]]), \n",
    "                                         len(y[name_dat[j][1]][0][list_combs[j][1]]))\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(len(list_combs)):\n",
    "        \n",
    "        #test data set\n",
    "        for period_good_good in list(range(list_periods[i][0][0], list_periods[i][0][1] - 1)):\n",
    "            if checker_n(period_good_good,data_use,i):\n",
    "                try:\n",
    "                    dater_test.append([[data_use[name_dat[i][0]][0][list_combs[i][0]][period_good_good],\n",
    "                                        data_use[name_dat[i][1]][0][list_combs[i][1]][period_good_good]], \n",
    "                                       [data_use[name_dat[i][0]][1][list_combs[i][0]][period_good_good],\n",
    "                                        data_use[name_dat[i][1]][1][list_combs[i][1]][period_good_good]],\n",
    "                                       met_dat[i][0]])\n",
    "                except:\n",
    "                    k += 1\n",
    "        #train data set\n",
    "        k_list = list(range(0,list_periods[i][1][0])) + list(range(list_periods[i][1][1], list_periods[i][1][2]))\n",
    "        for period_good_good in k_list :\n",
    "            if checker_n(period_good_good,data_use,i):\n",
    "                try: \n",
    "                    dater_train.append([[data_use[name_dat[i][0]][0][list_combs[i][0]][period_good_good],\n",
    "                                         data_use[name_dat[i][1]][0][list_combs[i][1]][period_good_good]],\n",
    "                                        [data_use[name_dat[i][0]][1][list_combs[i][0]][period_good_good],\n",
    "                                         data_use[name_dat[i][1]][1][list_combs[i][1]][period_good_good]],\n",
    "                                        met_dat[i][0]])\n",
    "                except:\n",
    "                    k += 1\n",
    "    print(str((k*100)/(len(list_periods)*list_periods[0][1][2])) + \"% of dates were skipped\")\n",
    "                                    \n",
    "    \n",
    "    return dater_train, dater_test\n",
    "\n",
    "def test_dates(bot_t, p_to_use):\n",
    "    return list(range(bot_t, p_to_use ))\n",
    "\n",
    "def train_dates(bot_tr, top_tr, max_p):\n",
    "    return list(range(bot_tr)) + list(range(top_tr, max_p))\n",
    "\n",
    "def pick_a_date(max_period, args_use):\n",
    "    \"\"\"\n",
    "    Giving max final period, return a breakdown of periods between training and testing\n",
    "    \"\"\"\n",
    "    ratio = args_use.ratio\n",
    "    #boundary values\n",
    "    validation_samples = min(max_period*(100-ratio), args_use.periods_testing) \n",
    "    \n",
    "    #test periods\n",
    "    p_use = rd.randint(validation_samples,max_period)\n",
    "    botim_test = p_use - validation_samples\n",
    "    \n",
    "    #train periods\n",
    "    top_train = p_use + args_use.periods_approximate\n",
    "    bottom_train = botim_test - args_use.periods_approximate\n",
    "    \n",
    "    return [[botim_test, p_use], [bottom_train, top_train, max_period]]\n",
    "\n",
    "def train_set(data_use, args):\n",
    "    \n",
    "    #how many periods \n",
    "    num_periods = list(data_use[0][0].values())[0].size()[0]\n",
    "    \n",
    "    #picking which asset combinations to use\n",
    "    num_combs = min(args.max_train, nChoosek(args.num_stocks_each,2))\n",
    "    all_keys = list(data_use[0][0].keys()) + list(data_use[1][0].keys())\n",
    "    which_combs = sample(list(combs(all_keys,2)), num_combs)\n",
    "    \n",
    "    \n",
    "    #how do we pick which periods to test on\n",
    "    if args.testing_dates == \"Random\":\n",
    "        #get a list of differnt periods for Testing and same dates for Trainig for all cases\n",
    "        p_use = [pick_a_date(num_periods, args) for i in range(num_combs)]\n",
    "    \n",
    "    else:\n",
    "        #get a list of the same periods for Testing and same dates for Trainig for all cases\n",
    "        p_use = np.repeat(pick_a_date(num_periods, args), num_combs)\n",
    "        \n",
    "    \n",
    "    #give define data sets\n",
    "    \n",
    "    \n",
    "    \n",
    "    return create_data_sets(p_use, which_combs, data_use)\n",
    "\n",
    "def set_up_tesnsors_as_neaded(tuple_of_setup):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_d, args, training_data_predefined = [], mod_use = None):\n",
    "    #setting up the packages\n",
    "    torch.set_num_threads(5)\n",
    "    npr.seed(args.seed)\n",
    "    dir_save_to = \"results/\" + args.dir_name\n",
    "    if not os.path.exists(dir_save_to):\n",
    "        os.makedirs(dir_save_to)\n",
    "    \n",
    "    \n",
    "    #type of metric to use for loss:\n",
    "    l_t = 1\n",
    "    if arg.loss_type == \"L2\":\n",
    "        l_t = 0\n",
    "    \n",
    "    \n",
    "    #which model to implament\n",
    "    if mod_use is None:\n",
    "        if args.model_use == \"Mod1\":\n",
    "              mod_use = TS_based_simple(num_outs = args.k_size) #args.num_filters, num_colours, arg.num_in_chans)\n",
    "        elif args.model_use == \"Mod1wFactors\":\n",
    "              mod_use = TS_based__wfactors(num_outs = args.k_size) #, args.num_filters, num_colours, arg.num_in_chans)\n",
    "        \n",
    "    #setting up the model's optimizaer\n",
    "    optimizer = torch.optim.Adam(mod_use.parameters(), lr=args.lrn_rate)\n",
    "    \n",
    "    #where we are going to gather data\n",
    "    hist_tr_loss = []\n",
    "    hist_tt_loss = []\n",
    "    \n",
    "    #creat trainig data sets if not premade\n",
    "    if training_data_predefined == []:\n",
    "        training_d, validation_d = train_set(train_dd, args)\n",
    "    else:\n",
    "        TS_d = training_data_predefined[0]\n",
    "        meta_d = training_data_predefined[1]\n",
    "        \n",
    "    #let's start training\n",
    "    for epoch in range(arg.num_epochs):\n",
    "        epoch_loss = []\n",
    "        \n",
    "        \n",
    "        #training data\n",
    "        for ddd in TS_d:\n",
    "            input_d = ddd[0]\n",
    "            #a.unsqueeze_(-1)\n",
    "            #a = a.expand(3,3,10)\n",
    "            dims_want_in = tuple(list(input_d[0].size()) + [1])\n",
    "            print(input_d[0])\n",
    "            for tens_up in range(len(input_d)):\n",
    "                input_d[tens_up].unsqueeze_(-1)\n",
    "                input_d[tens_up] = input_d[tens_up].expand(dims_want_in[:2])\n",
    "            print(input_d[0])\n",
    "            input_d.unsqueeze_(-1)\n",
    "            input_d = input_d.expand(dims_want_in)\n",
    "            print(input_d)\n",
    "            \n",
    "            output_d = ddd[1][0]\n",
    "            dims_I_want_out = tuple(list(output_d[0].size()) + [1])\n",
    "            #print(output_d)\n",
    "            \n",
    "            #setup optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #set up optimization\n",
    "            output_choice = mod_use(input_d)\n",
    "            CM_use = which_mat(input_d, output_choice)\n",
    "            lossing_me = cov_matrix_loss(output_d, CM_use, l_t)\n",
    "            \n",
    "            #optimizing\n",
    "            lossing_me.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #adding performance\n",
    "            epoch_loss.append(lossing.data.item())\n",
    "        \n",
    "        #printing performance on epoch\n",
    "        mean_loss = np.mean(epoch_loss)\n",
    "        hist_tr_loss.append(mean_loss)\n",
    "        print('Epoch [%d/%d], T Loss: %.4f' % (epoch+1, args.num_epochs, mean_loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #let's validate\n",
    "        temp_validation = []\n",
    "        for ddd in validation_d:\n",
    "            inputs = ddd[0]\n",
    "            meta_d = ddd[1]\n",
    "            \n",
    "            input_d = inputs[0]\n",
    "            outpt_d = inputs[1]\n",
    "            \n",
    "            #setup optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #set up optimization\n",
    "            output_choice = mod_use(input_d)\n",
    "            CM_use = which_mat(input_d, output_choice)\n",
    "            lossing_me = cov_matrix_loss(output_d, CM_use, l_t)\n",
    "            \n",
    "            \n",
    "            #adding performance\n",
    "            temp_validation.append(lossing.data.item())\n",
    "        \n",
    "        hist_tt_loss = []\n",
    "        #printing performance on epoch\n",
    "        mean_loss = np.mean(temp_validation)\n",
    "        hist_tt_loss.append(mean_loss)\n",
    "        print('Epoch [%d/%d], V Loss: %.4f' % (epoch+1, args.num_epochs, mean_loss))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(hist_tr_loss, \"ro-\", label=\"Train\")\n",
    "    plt.plot(hist_tt_loss, \"go-\", label=\"Validation\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.savefig(save_dir+\"/training_curve.png\")\"\"\"\n",
    "\n",
    "    if args.checkpoint:\n",
    "        print('Saving model...')\n",
    "        torch.save(mod_use.state_dict(), args.save_model_as)\n",
    "    \n",
    "    return mod_use\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
