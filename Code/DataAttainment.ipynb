{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Financial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re, os\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl as qdl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import abspath\n",
    "from inspect import getsourcefile\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import BDay\n",
    "from pandas.tseries.offsets import CustomBusinessDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_iterator(rootdir):\n",
    "    files = []\n",
    "    f = []\n",
    "    g = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == \"csv\":\n",
    "                f.append(os.path.join(subdir, file))\n",
    "                g.append(file[:-4])\n",
    "    return(f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re,os\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl as qdl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import abspath\n",
    "from inspect import getsourcefile\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import BDay\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "def file_iterator(rootdir):\n",
    "    files = []\n",
    "    f = []\n",
    "    g = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == \"csv\":\n",
    "                f.append(os.path.join(subdir, file))\n",
    "                g.append(file[:-4])\n",
    "    return(f, g)\n",
    "  \n",
    "  \n",
    "class data:\n",
    "    def __init__(self, quandl_key, data_type = [\"SP400\"], s_d = dt.today(), e_d = dt(2018, 3, 27, 0, 0), from_end =  3*365):\n",
    "        \n",
    "        self.qdl_key = quandl_key\n",
    "        self.end_date = e_d\n",
    "        self.data_type = data_type\n",
    "\n",
    "        if s_d < e_d :\n",
    "            self.start_date = s_d\n",
    "        else:\n",
    "            self.start_date = self.end_date - td(days = from_end)\n",
    "        \n",
    "    def get_data_daily_rand(self , num_use = 10000, path_use = \"\",column_name = \"Ticker\"):\n",
    "      #column_name = \"ACT Symbol\" for SP500\n",
    "      #column_name = \"Ticker\" for SP400\n",
    "        \n",
    "        qdl.ApiConfig.api_key = self.qdl_key\n",
    "        \n",
    "        if path_use == \"\":\n",
    "            pl = abspath('/content/csc421/project/data')+'/'\n",
    "        else:\n",
    "            pl = abspath(path_use)\n",
    "        \n",
    "        cd = re.split(r'\\<', pl)[0]\n",
    "        print(cd)\n",
    "        t = 0\n",
    "        splitty = 0\n",
    "        f_it = file_iterator(cd)\n",
    "        for j in f_it[0]:\n",
    "            if f_it[1][splitty] in self.data_type:\n",
    "                tickers = pd.read_csv(j)[column_name]\n",
    "                num_samp = min(len(tickers), num_use)\n",
    "                tickers = tickers[np.random.choice(len(tickers),num_samp, replace = False)]\n",
    "                tickers = tickers\n",
    "                for i in tickers:\n",
    "                    t += 1\n",
    "                    beg_d = self.start_date.strftime(\"%Y-%m-%d\")\n",
    "                    end_d = self.end_date.strftime(\"%Y-%m-%d\")\n",
    "                    if t == 1 :\n",
    "                        data_use = qdl.get_table(\"WIKI/PRICES\",\n",
    "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
    "                                   ticker= i, \n",
    "                                   date = {'gte': beg_d,'lte' : end_d})\n",
    "                        data_use = data_use.set_index([\"date\"], drop=True)    \n",
    "\n",
    "                    else:\n",
    "                        temp = qdl.get_table(\"WIKI/PRICES\",\n",
    "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
    "                                   ticker= i, \n",
    "                                   date = {'gte': beg_d,'lte' : end_d})\n",
    "                        temp = temp.set_index([\"date\"], drop=True)\n",
    "\n",
    "                        tot = [data_use, temp]\n",
    "                        data_use = pd.concat(tot)\n",
    "                \n",
    "                if (t/num_samp *100 % 20) == 0:\n",
    "                    print(t/len(tickers))\n",
    "        \n",
    "            splitty += 1\n",
    "            print(splitty/len(f_it[0]))\n",
    "        self.data_got_rand = data_use \n",
    "        \n",
    "    def get_data_daily_specific(self, ticker_use):\n",
    "      \n",
    "        self.ticker_main = ticker_use\n",
    "        self.ticker_main = ticker_use\n",
    "        qdl.ApiConfig.api_key = self.qdl_key\n",
    "\n",
    "        beg_d = self.start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_d = self.end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        data_use = qdl.get_table(\"WIKI/PRICES\",\n",
    "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
    "                                    ticker= self.ticker_main, \n",
    "                                   date = {'gte': beg_d,'lte' : end_d})\n",
    "        self.data_got_specific = data_use.set_index([\"date\"], drop=True)\n",
    "        \n",
    "        \n",
    "    def set_up_features(self, which_data = \"rand\",ratio = [60,30,10],num_features = [30], grouping_feature = [\"ticker\"],\n",
    "                        TS_feature_set = [\"close\",\"open\",\"low\",\"high\"], cat_feature = [], order = []):\n",
    "        \"\"\"\n",
    "        \n",
    "        Will need to add catigorcial feature later for improved implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        if not (sum(ratio) == 100):\n",
    "            return \"ratio entered is invalid\"\n",
    "        \n",
    "        if which_data == \"rand\":\n",
    "            training_d = self.data_got_rand\n",
    "        \n",
    "        elif which_data == \"specific\":\n",
    "            training_d = self.data_got_specific\n",
    "            \n",
    "        training_d = training_d[TS_feature_set + grouping_feature]\n",
    "        dd = training_d.groupby(grouping_feature)\n",
    "        \n",
    "        \n",
    "        def make_d(list_x):\n",
    "            b = list_x[0]\n",
    "            for c in list_x[1:]:\n",
    "                b = pd.concat([b.reset_index(), c.reset_index()], \n",
    "                              axis =1 ).drop('date', axis = 1).set_index(b.index)\n",
    "            return b\n",
    "        \n",
    "        #unnecessary code\n",
    "        #for feature_type in TS_feature_set:\n",
    "        #    dd_get = {a:[group[feature_type][i : - num_features[0] + 1 + i] \n",
    "        #               for i in range(num_features[0])] for a,group in dd}    \n",
    "         \n",
    "        self.dd = {a:[[group[feature_type][i : - num_features[0] + i]\n",
    "                       for i in range(num_features[0])]\n",
    "                      for feature_type in TS_feature_set]\n",
    "                   for a,group in dd} \n",
    "        #torch.t(torch.FloatTensor(S_P_Data.dd['AIG'][0][0:29]))\n",
    "        dd_wanted = {}\n",
    "        for a in self.dd.keys():\n",
    "          w = torch.t(torch.FloatTensor(self.dd[a][0][0:num_features[0]])).unsqueeze(-1)\n",
    "          x = torch.t(torch.FloatTensor(self.dd[a][1][0:num_features[0]])).unsqueeze(-1)\n",
    "          y = torch.t(torch.FloatTensor(self.dd[a][2][0:num_features[0]])).unsqueeze(-1)\n",
    "          z = torch.t(torch.FloatTensor(self.dd[a][3][0:num_features[0]])).unsqueeze(-1)\n",
    "          aa = torch.cat((w, x, y, z), 2)\n",
    "          dd_wanted[a] = aa\n",
    "          #) for a in dd.keys()}\n",
    "            \n",
    "        self.data_cleaned = dd_wanted\n",
    "        \n",
    "    def as_tensor(self, value):\n",
    "        torch.tensor(np.asanyarray(self.value))\n",
    "        \n",
    "    def get_wide(self, price_type):\n",
    "        self.d_wide = data_got_rand.pivot(columns=\"ticker\", values= price_type)\n",
    "        \n",
    "def save_to_pkl(path, objectname, endname):\n",
    "  with open(os.path.join(path, endname), 'wb') as f:\n",
    "    pkl.dump(objectname, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
