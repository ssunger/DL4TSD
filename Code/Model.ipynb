{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Modelling for the TS based Cov-Matrix Picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import random as npr\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_mat(TTS, choice, n = [7, 7*2, 7*4, 7*4*2, 7*4*3], p = [0], channel_use = 1):\n",
    "    \"\"\"\n",
    "    Funciton which can be easilly calle din order to implament which_mat as part of a larger function\n",
    "    \"\"\"\n",
    "    return get_mat(TTS, n[choice[0]], p[choice[1]], channel_use)\n",
    "\n",
    "def get_mat(TTS, n,p = 0, channel_use = 1):\n",
    "    \"\"\"\n",
    "    Funciton that takes in \n",
    "    TTS Tesnor of Time Serieses, and given \n",
    "    n number of period to use and\n",
    "    p  last index period to use, \n",
    "    return a covariance matrix of the data for that period in terms of the values in chanel channel_use\n",
    "    \"\"\"\n",
    "    \n",
    "    d1, d2 = TTS[0], TTS[1]\n",
    "    #insert function to partition TTS correctly\n",
    "    \n",
    "    #creating cova matrix estimation desired\n",
    "    d = [d1, d1]\n",
    "    return pd.DataFrame({\"S%d\" %i: d for i,d in  enumerate(d)}).cov()\n",
    "\n",
    "\n",
    "def cov_matrix_loss(A,B, type_use = 1):\n",
    "    \"\"\"\n",
    "    Get distance between matracies based either on component wise distance or using eigenvalues\n",
    "    \"\"\"\n",
    "    if type_use == 0:\n",
    "        ind = np.triu_indices(2)\n",
    "        loss = nn.MSELoss()\n",
    "        return loss(A[ind], B[ind]) \n",
    "    \n",
    "    elif type_use == 1:\n",
    "        return torch.sqrt(torch.sum(torch.pow(torch.log(torch.eig(A-B))),2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_based_simple(nn.Module):\n",
    "    def __init__(self, num_outs,\n",
    "                 kernel = 5, in_channels = 4,  hidden_size = 128):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_simple, self).__init__()\n",
    "        \n",
    "        pads = kernel // 2 \n",
    "        \n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = hidden_size, \n",
    "                      stride = 2,kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = (hidden_size/2), \n",
    "                      kernel_size = (kernel-2), stride = 2, padding = (pads-1) ),\n",
    "            nn.BatchNorm1d(hidden_size/2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = (hidden_size/2), out_channels = (hidden_size/(2**2)), \n",
    "                      kernel_size = (kernel-2), stride = 2, padding = (pads-1) ),\n",
    "            nn.BatchNorm1d(hidden_size/(2**2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconvLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = (hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = (kernel-2), padding = (pads-1)),\n",
    "            nn.BatchNorm1d(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d()\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(out_channels = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.cov1(input_1)\n",
    "        self.c12 = self.cov2(self.c11)\n",
    "        self.c13 = self.cov3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.cov1(input_2)\n",
    "        self.c22 = self.cov2(self.c11)\n",
    "        self.c23 = self.cov3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        \n",
    "\n",
    "class TS_based_wfactors(nn.Module):\n",
    "    def __init__(self, num_outs, kernel = 5, channels_in =4, hidden_size = 128):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_wfactors, self).__init__()\n",
    "        \n",
    "        pads  == kernel//2\n",
    "        \n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = channels_in, out_channels = hidden_size, \n",
    "                      stride = 2,kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = (hidden_size/2), \n",
    "                      kernel_size = (kernel-2), stride = 2, padding = (pads-1) ),\n",
    "            nn.BatchNorm1d(hidden_size/2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = (hidden_size/2), out_channels = (hidden_size/(2**2)), \n",
    "                      kernel_size = (kernel-2), stride = 2, padding = (pads-1)),\n",
    "            nn.BatchNorm1d(hidden_size/(2**2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconvLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = (hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = (kernel-2), padding = (pads-1)),\n",
    "            nn.BatchNorm1d(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d()\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(in_channels = num_outs , out_channels = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.cov1(input_1)\n",
    "        self.c12 = self.cov2(self.c11)\n",
    "        self.c13 = self.cov3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.cov1(input_2)\n",
    "        self.c22 = self.cov2(self.c11)\n",
    "        self.c23 = self.cov3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set(data_use, max_num_training):\n",
    "    pop\n",
    "\n",
    "def train(train_d, args, mod_use = None):\n",
    "    #setting up the packages\n",
    "    torch.set_num_threads(5)\n",
    "    npr.seed(args.seed)\n",
    "    dir_save_to = \"results/\" + args.dir_name\n",
    "    if not os.path.exists(dir_save_to):\n",
    "        os.makedirs(dir_save_to)\n",
    "    \n",
    "    \n",
    "    #type of metric to use for loss:\n",
    "    l_t = 1\n",
    "    if arg.loss_type == \"L2\":\n",
    "        l_t = 0\n",
    "    \n",
    "    \n",
    "    #which model to implament\n",
    "    if mod_use is None:\n",
    "        if args.model_use == \"Mod1\":\n",
    "              mod_use = TS_based_simple(num_outs = args.k_size) #args.num_filters, num_colours, arg.num_in_chans)\n",
    "        elif args.model_use == \"Mod1wFactors\":\n",
    "              mod_use = TS_based__wfactors(num_outs = args.k_size) #, args.num_filters, num_colours, arg.num_in_chans)\n",
    "    \n",
    "    #setting up the model's optimizaer\n",
    "    optimizer = torch.optim.Adam(mod_use.parameters(), lr=args.lrn_rate)\n",
    "    \n",
    "    #where we are going to gather data\n",
    "    hist_tr_loss = []\n",
    "    hist_tt_loss = []\n",
    "    \n",
    "    #let's start training\n",
    "    for i in range(arg.num_epochs):\n",
    "        epoch_loss = []\n",
    "        \n",
    "        for input_d in training_set(train_d, args.max_train):\n",
    "            #setup optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #set up optimization\n",
    "            output_choice = mod_use(input_d)\n",
    "            CM_use = which_mat(output_choice)\n",
    "            lossing_me = cov_matrix_loss(output_chice, CM_use, l_t)\n",
    "            \n",
    "            #optimizing\n",
    "            lossing.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(lossing.data.item())\n",
    "            \n",
    "        \n",
    "        #printing some stuff\n",
    "        avg_loss = np.mean(losses)\n",
    "        train_losses.append(avg_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SP400.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8de0f4734b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SP400.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SP400.pkl'"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"SP400.pkl\",\"rb\")\n",
    "pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
