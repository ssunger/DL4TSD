{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Modelling for the TS based Cov-Matrix Picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import factorial as fac\n",
    "import matplotlib\n",
    "from random import sample \n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import combinations as combs\n",
    "from numpy import random as npr\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nChoosek(n,k):\n",
    "    return fac(n) // fac(r) // fac(n-r)\n",
    "\n",
    "def which_mat(TTS, choice, n = [5, 5*3, 5*5, 5*9, 5*13], p = [0], channel_use = 1):\n",
    "    \"\"\"\n",
    "    Funciton which can be easilly calle din order to implament which_mat as part of a larger function\n",
    "    \"\"\"\n",
    "    return get_mat(TTS, n[choice[0]], p[0], channel_use)\n",
    "\n",
    "def get_mat(TTS, n,p = 0, channel_use = 1):\n",
    "    \"\"\"\n",
    "    Funciton that takes in \n",
    "    TTS Tesnor of Time Serieses, and given \n",
    "    n number of period to use and\n",
    "    p  last index period to use, \n",
    "    return a covariance matrix of the data for that period in terms of the values in chanel channel_use\n",
    "    \"\"\"\n",
    "    \n",
    "    d1, d2 = TTS[0], TTS[1]\n",
    "    #insert function to partition TTS correctly\n",
    "    \n",
    "    #creating cova matrix estimation desired\n",
    "    d = [d1, d1]\n",
    "    return pd.DataFrame({\"S%d\" %i: d for i,d in  enumerate(d)}).cov()\n",
    "\n",
    "\n",
    "def cov_matrix_loss(A,B, type_use = 1):\n",
    "    \"\"\"\n",
    "    Get distance between matracies based either on component wise distance or using eigenvalues\n",
    "    \"\"\"\n",
    "    if type_use == 0:\n",
    "        ind = np.triu_indices(2)\n",
    "        loss = nn.MSELoss()\n",
    "        return loss(A[ind], B[ind]) \n",
    "    \n",
    "    elif type_use == 1:\n",
    "        return torch.sqrt(torch.sum(torch.pow(torch.log(torch.eig(A-B))),2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_based_simple(nn.Module):\n",
    "    def __init__(self, num_outs,\n",
    "                 kernel = 5, in_channels = 4,  hidden_size = 128):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_simple, self).__init__()\n",
    "        \n",
    "        pads = int(kernel // 2)\n",
    "        \n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = hidden_size, \n",
    "                      stride = 2,kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = int(hidden_size/2), \n",
    "                      kernel_size = int(kernel-2), stride = 2, padding = int(pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/2), out_channels = int(hidden_size/(2**2)), \n",
    "                      kernel_size = int(kernel-2), stride = 2, padding = (pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/(2**2))),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconvLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(num_outs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(in_features = num_outs, out_features = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.cov1(input_1)\n",
    "        self.c12 = self.cov2(self.c11)\n",
    "        self.c13 = self.cov3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.cov1(input_2)\n",
    "        self.c22 = self.cov2(self.c11)\n",
    "        self.c23 = self.cov3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        \n",
    "\n",
    "class TS_based_wfactors(nn.Module):\n",
    "    def __init__(self, num_outs, kernel = 5, channels_in =4, hidden_size = 128):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TS_based_wfactors, self).__init__()\n",
    "        \n",
    "        pads  = int(kernel//2)\n",
    "        \n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = channels_in, out_channels = hidden_size, \n",
    "                      stride = 2,kernel_size = kernel, padding = pads ),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size, out_channels = int(hidden_size/2), \n",
    "                      kernel_size = int(kernel-2), stride = 2, padding = int(pads-1) ),\n",
    "            nn.BatchNorm1d(int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/2), out_channels = int(hidden_size/(2**2)), \n",
    "                      kernel_size = int(kernel-2), stride = 2, padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(int(hidden_size/(2**2))),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.downconvLast = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = int(hidden_size/(2**2)) , out_channels = num_outs, \n",
    "                      kernel_size = int(kernel-2), padding = int(pads-1)),\n",
    "            nn.BatchNorm1d(num_outs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "            )\n",
    "        \n",
    "        self.picker = nn.Linear(in_channels = num_outs , out_channels = num_outs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_1, input_2 = input[0], input[1]\n",
    "        \n",
    "        #run the model on the first TS\n",
    "        self.c11 = self.cov1(input_1)\n",
    "        self.c12 = self.cov2(self.c11)\n",
    "        self.c13 = self.cov3(self.c12)\n",
    "        \n",
    "        #run the model on the second TS\n",
    "        self.c21 = self.cov1(input_2)\n",
    "        self.c22 = self.cov2(self.c11)\n",
    "        self.c23 = self.cov3(self.c12)\n",
    "        \n",
    "        #concat the processed data and downconvthat\n",
    "        self.cl = self.downconvLast(torch.cat((self.c23, self.c3), dim=1))\n",
    "        \n",
    "        #concat TS based data to data representative of other info\n",
    "        self.last = self.picker(self.cl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_set(data_use, args):\n",
    "    \n",
    "    #how many periods \n",
    "    num_periods = list(data_use[0].values())[0].size()[0]\n",
    "    \n",
    "    #picking which asset combinations to use\n",
    "    num_combs = min(args.max_train, nChoosek(args.num_stocks_each,2))\n",
    "    all_keys = list(data_use[0].keys()) + list(data_use[1].keys())\n",
    "    which_combs = sample(list(combs(all_keys,2)), num_combs)\n",
    "    \n",
    "    \n",
    "    #how do we pick which periods to test on\n",
    "    if args.testing_dates == \"Random\":\n",
    "        na\n",
    "    else:\n",
    "        nada\n",
    "        \n",
    "    \n",
    "    #get training and testing data sets\n",
    "    \n",
    "    \n",
    "    return d_t, d_v\n",
    "\n",
    "def train(train_d, args, mod_use = None):\n",
    "    #setting up the packages\n",
    "    torch.set_num_threads(5)\n",
    "    npr.seed(args.seed)\n",
    "    dir_save_to = \"results/\" + args.dir_name\n",
    "    if not os.path.exists(dir_save_to):\n",
    "        os.makedirs(dir_save_to)\n",
    "    \n",
    "    \n",
    "    #type of metric to use for loss:\n",
    "    l_t = 1\n",
    "    if arg.loss_type == \"L2\":\n",
    "        l_t = 0\n",
    "    \n",
    "    \n",
    "    #which model to implament\n",
    "    if mod_use is None:\n",
    "        if args.model_use == \"Mod1\":\n",
    "              mod_use = TS_based_simple(num_outs = args.k_size) #args.num_filters, num_colours, arg.num_in_chans)\n",
    "        elif args.model_use == \"Mod1wFactors\":\n",
    "              mod_use = TS_based__wfactors(num_outs = args.k_size) #, args.num_filters, num_colours, arg.num_in_chans)\n",
    "    \n",
    "    #setting up the model's optimizaer\n",
    "    optimizer = torch.optim.Adam(mod_use.parameters(), lr=args.lrn_rate)\n",
    "    \n",
    "    #where we are going to gather data\n",
    "    hist_tr_loss = []\n",
    "    hist_tt_loss = []\n",
    "    \n",
    "    #let's start training\n",
    "    for epoch in range(arg.num_epochs):\n",
    "        epoch_loss = []\n",
    "        \n",
    "        #let's train\n",
    "        training_d, validation_d = train_set(train_d, args)\n",
    "        for input_d, output_d, meta_d in training_d:\n",
    "            #setup optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #set up optimization\n",
    "            output_choice = mod_use(input_d)\n",
    "            CM_use = which_mat(input_d, output_choice)\n",
    "            lossing_me = cov_matrix_loss(output_chice, CM_use, l_t)\n",
    "            \n",
    "            #optimizing\n",
    "            lossing_me.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #adding performance\n",
    "            epoch_loss.append(lossing.data.item())\n",
    "        \n",
    "        #printing performance on epoch\n",
    "        mean_loss = np.mean(epoch_loss)\n",
    "        hist_tr_loss.append(mean_loss)\n",
    "        print('Epoch [%d/%d], T Loss: %.4f' % (epoch+1, args.num_epochs, mean_loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #let's validate\n",
    "        temp_validation = []\n",
    "        for input_d in validation_d:\n",
    "            #setup optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #set up optimization\n",
    "            output_choice = mod_use(input_d)\n",
    "            CM_use = which_mat(output_choice)\n",
    "            lossing_me = cov_matrix_loss(output_chice, CM_use, l_t)\n",
    "            \n",
    "            \n",
    "            #adding performance\n",
    "            temp_validation.append(lossing.data.item())\n",
    "        \n",
    "        hist_tt_loss = []\n",
    "        #printing performance on epoch\n",
    "        mean_loss = np.mean(temp_validation)\n",
    "        hist_tt_loss.append(mean_loss)\n",
    "        print('Epoch [%d/%d], V Loss: %.4f' % (epoch+1, args.num_epochs, mean_loss))\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(hist_tt_loss, \"ro-\", label=\"Train\")\n",
    "    plt.plot(hist_tr_loss, \"go-\", label=\"Validation\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.savefig(save_dir+\"/training_curve.png\")\n",
    "\n",
    "    if args.checkpoint:\n",
    "        print('Saving model...')\n",
    "        torch.save(mod_use.state_dict(), args.save_model_as)\n",
    "    \n",
    "    return mod_use\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
