{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-Project-20190416.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kYioKz2TMN5H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Neural Nets - Final Project"
      ]
    },
    {
      "metadata": {
        "id": "OuSNtHIpLis6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Loading Modules"
      ]
    },
    {
      "metadata": {
        "id": "FnEDz3KPLe73",
        "colab_type": "code",
        "outputId": "6ea8d679-f330-45a1-dbcb-d449dc58c953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install quandl\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/project/\n",
        "%cd /content/csc421/project"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/c2/f84b1e57416755e967236468dcfb0fad7fd911f707185efc4ba8834a1a94/Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-6.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quandl in /usr/local/lib/python3.6/dist-packages (3.4.6)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.18.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.16.2)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.23.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.22)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mscikit-image 0.14.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 6.0.0\n",
            "    Uninstalling Pillow-6.0.0:\n",
            "      Successfully uninstalled Pillow-6.0.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jizzi5ODLlO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "V96-JytuLs9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZyV3b_ESZcy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Getting TS Data from Quandl"
      ]
    },
    {
      "metadata": {
        "id": "xCiQTMULg9O1",
        "colab_type": "code",
        "outputId": "a74b1261-4951-470b-f77a-bd8ad39c7af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download ticker list from Github\n",
        "######################################################################\n",
        "#data_fpath = get_file(fname='SP500.csv', \n",
        "#                         origin='https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP500.csv', \n",
        "#                         untar=False)\n",
        "#data_fpath = get_file(fname='SP400.csv', \n",
        "#                         origin='https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP400.csv', \n",
        "#                         untar=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/SP500.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-L8HLHsIP3e1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, re,os\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta as td\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import quandl as qdl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os.path import abspath\n",
        "from inspect import getsourcefile\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import BDay\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "\n",
        "def file_iterator(rootdir):\n",
        "    files = []\n",
        "    f = []\n",
        "    g = []\n",
        "    for subdir, dirs, files in os.walk(rootdir):\n",
        "        for file in files:\n",
        "            if file[-3:] == \"csv\":\n",
        "                f.append(os.path.join(subdir, file))\n",
        "                g.append(file[:-4])\n",
        "    return(f, g)\n",
        "  \n",
        "  \n",
        "class data:\n",
        "    def __init__(self, quandl_key, data_type = [\"SP400\"], s_d = dt.today(), e_d = dt(2018, 3, 27, 0, 0), from_end =  3*365):\n",
        "        \n",
        "        self.qdl_key = quandl_key\n",
        "        self.end_date = e_d\n",
        "        self.data_type = data_type\n",
        "\n",
        "        if s_d < e_d :\n",
        "            self.start_date = s_d\n",
        "        else:\n",
        "            self.start_date = self.end_date - td(days = from_end)\n",
        "        \n",
        "    def get_data_daily_rand(self , num_use = 10000, path_use = \"\",column_name = \"Ticker\"):\n",
        "      #column_name = \"ACT Symbol\" for SP500\n",
        "      #column_name = \"Ticker\" for SP400\n",
        "        \n",
        "        qdl.ApiConfig.api_key = self.qdl_key\n",
        "        \n",
        "        if path_use == \"\":\n",
        "            pl = abspath('/content/csc421/project/data')+'/'\n",
        "        else:\n",
        "            pl = path_use\n",
        "\n",
        "        cd = re.split(r'\\<', pl)[0]\n",
        "        t = 0\n",
        "        splitty = 0\n",
        "        f_it = file_iterator(cd)\n",
        "        for j in f_it[0]:\n",
        "            if f_it[1][splitty] in self.data_type:\n",
        "                tickers = pd.read_csv(j)[column_name]\n",
        "                num_samp = min(len(tickers), num_use)\n",
        "                tickers = tickers[np.random.choice(len(tickers),num_samp, replace = False)]\n",
        "                tickers = tickers\n",
        "                for i in tickers:\n",
        "                    t += 1\n",
        "                    beg_d = self.start_date.strftime(\"%Y-%m-%d\")\n",
        "                    end_d = self.end_date.strftime(\"%Y-%m-%d\")\n",
        "                    if t == 1 :\n",
        "                        data_use = qdl.get_table(\"WIKI/PRICES\",\n",
        "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
        "                                   ticker= i, \n",
        "                                   date = {'gte': beg_d,'lte' : end_d})\n",
        "                        data_use = data_use.set_index([\"date\"], drop=True)    \n",
        "\n",
        "                    else:\n",
        "                        temp = qdl.get_table(\"WIKI/PRICES\",\n",
        "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
        "                                   ticker= i, \n",
        "                                   date = {'gte': beg_d,'lte' : end_d})\n",
        "                        temp = temp.set_index([\"date\"], drop=True)\n",
        "\n",
        "                        tot = [data_use, temp]\n",
        "                        data_use = pd.concat(tot)\n",
        "                \n",
        "                if (t/num_samp *100 % 20) == 0:\n",
        "                    print(t/len(tickers))\n",
        "        \n",
        "            splitty += 1\n",
        "            print(splitty/len(f_it[0]))\n",
        "        self.data_got_rand = data_use \n",
        "        \n",
        "    def get_data_daily_specific(self, ticker_use):\n",
        "      \n",
        "        self.ticker_main = ticker_use\n",
        "        self.ticker_main = ticker_use\n",
        "        qdl.ApiConfig.api_key = self.qdl_key\n",
        "\n",
        "        beg_d = self.start_date.strftime(\"%Y-%m-%d\")\n",
        "        end_d = self.end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        data_use = qdl.get_table(\"WIKI/PRICES\",\n",
        "                                   qopts={\"columns\":[\"date\",\"ticker\",\"open\",\"low\",\"high\",\"close\",\"adj_close\",\"volume\"]},\n",
        "                                    ticker= self.ticker_main, \n",
        "                                   date = {'gte': beg_d,'lte' : end_d})\n",
        "        self.data_got_specific = data_use.set_index([\"date\"], drop=True)\n",
        "        \n",
        "        \n",
        "    def set_up_features(self, which_data = \"rand\",ratio = [60,30,10],num_features = [30], grouping_feature = [\"ticker\"],\n",
        "                        TS_feature_set = [\"close\",\"open\",\"low\",\"high\"], cat_feature = [], order = []):\n",
        "        \"\"\"\n",
        "        \n",
        "        Will need to add catigorcial feature later for improved implementation\n",
        "        \"\"\"\n",
        "        \n",
        "        if not (sum(ratio) == 100):\n",
        "            return \"ratio entered is invalid\"\n",
        "        \n",
        "        if which_data == \"rand\":\n",
        "            training_d = self.data_got_rand\n",
        "        \n",
        "        elif which_data == \"specific\":\n",
        "            training_d = self.data_got_specific\n",
        "            \n",
        "        training_d = training_d[TS_feature_set + grouping_feature]\n",
        "        dd = training_d.groupby(grouping_feature)\n",
        "        \n",
        "        \n",
        "        def make_d(list_x):\n",
        "            b = list_x[0]\n",
        "            for c in list_x[1:]:\n",
        "                b = pd.concat([b.reset_index(), c.reset_index()], \n",
        "                              axis =1 ).drop('date', axis = 1).set_index(b.index)\n",
        "            return b\n",
        "        \n",
        "        #unnecessary code\n",
        "        #for feature_type in TS_feature_set:\n",
        "        #    dd_get = {a:[group[feature_type][i : - num_features[0] + 1 + i] \n",
        "        #               for i in range(num_features[0])] for a,group in dd}    \n",
        "         \n",
        "        self.dd = {a:[[group[feature_type][i : - num_features[0] + 1 + i]\n",
        "                       for i in range(num_features[0])]\n",
        "                      for feature_type in TS_feature_set]\n",
        "                   for a,group in dd} \n",
        "        #torch.t(torch.FloatTensor(S_P_Data.dd['AIG'][0][0:29]))\n",
        "        dd_wanted = {}\n",
        "        for a in self.dd.keys():\n",
        "          w = torch.t(torch.FloatTensor(self.dd[a][0][0:29])).unsqueeze(-1)\n",
        "          x = torch.t(torch.FloatTensor(self.dd[a][1][0:29])).unsqueeze(-1)\n",
        "          y = torch.t(torch.FloatTensor(self.dd[a][2][0:29])).unsqueeze(-1)\n",
        "          z = torch.t(torch.FloatTensor(self.dd[a][3][0:29])).unsqueeze(-1)\n",
        "          aa = torch.cat((w, x, y, z), 2)\n",
        "          dd_wanted[a] = aa\n",
        "          #) for a in dd.keys()}\n",
        "            \n",
        "        self.data_cleaned = dd_wanted\n",
        "        \n",
        "    def as_tensor(self, value):\n",
        "        torch.tensor(np.asanyarray(self.value))\n",
        "        \n",
        "    def get_wide(self, price_type):\n",
        "        self.d_wide = data_got_rand.pivot(columns=\"ticker\", values= price_type)\n",
        "        \n",
        "def save_to_pkl(path, objectname, endname):\n",
        "  with open(os.path.join(path, endname), 'wb') as f:\n",
        "    pkl.dump(objectname, f)\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHtEMmiIRHrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "qdl_key_implament = \"zFoL6yQsQfFgifpzPQFC\"\n",
        "S_P_Data = data(qdl_key_implament, from_end=8*365) #8*365 for 8 years back"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gN21KptHQPvY",
        "colab_type": "code",
        "outputId": "4b9f1bcb-0c0b-4f4d-b338-1310a5be8af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "S_P_Data.get_data_daily_rand(num_use = 100)\n",
        "S_P_Data.set_up_features()\n",
        "\n",
        "\n",
        "### Saving procured dataset as pickle\n",
        "save_to_pkl('/content/csc421/project/data/', S_P_Data.data_cleaned, 'SP400.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PtsasCTgjbXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Opening pickle dataset\n",
        "\n",
        "### if opening from Colab env:\n",
        "#file_pi1 = open('/content/csc421/project/data/SP400.pkl', 'rb') \n",
        "\n",
        "### opening from Github:\n",
        "#file_pi1 = open('/https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP400.pkl', 'rb')\n",
        "#SP400data = pickle.load(file_pi1)\n",
        "#file_pi2 = open('/https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP500.pkl', 'rb')\n",
        "#SP500data = pickle.load(file_pi2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5QPW8r2kThR",
        "colab_type": "code",
        "outputId": "1239a16d-fd3d-4f82-ecd4-b5d7cb18a4a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download above processed Pickled data from Github\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='SP500.pkl', \n",
        "                         origin='/https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP500.pkl', \n",
        "                         untar=False)\n",
        "data_fpath = get_file(fname='SP400.pkl', \n",
        "                         origin='/https://raw.githubusercontent.com/ssunger/DL4TSD/master/Data%20Sets/Constituent%20Data/SP400.pkl', \n",
        "                         untar=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/SP500.pkl\n",
            "data/SP400.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-RZdYammj9M4",
        "colab_type": "code",
        "outputId": "3d5686a2-649a-48e6-8315-1a68867c63c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "### Tickers for which we have extracted data\n",
        "S_P_Data.data_cleaned.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([222, 29, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "metadata": {
        "id": "z_ZOcV-7Ml8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training"
      ]
    },
    {
      "metadata": {
        "id": "bbVitXk2M1eS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', \n",
        "              'attention_type': '', \n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "#translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "#print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}